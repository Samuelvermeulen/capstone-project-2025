

"""
Feature Engineering Module - Phase 4
Samuel Vermeulen - Capstone Project 2025

Objectif : Construire progressivement les features selon le roadmap
Approche : Une fonction par √©tape, test√©e individuellement
"""

import pandas as pd
import numpy as np
import logging
from typing import Tuple

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Liste bas√©e sur le 5-Year Ranking 2023, avec les noms EXACTS du dataset
TOP_CLUBS_EURO_RANKING = [
    'Manchester City',  # Rank 1
    'Liverpool',        # Rank 2
    'Chelsea',          # Rank 7
    'Manchester Utd',   # Rank 10 (ATTENTION: diff√©rent de 'Manchester United')
    'Arsenal',          # Rank 15
    'Tottenham',        # Rank 16
    'Leicester City',   # Rank 33
    'West Ham',         # Rank 38
    'Wolves',           # Rank 47 (Nom exact du dataset)
    'Newcastle Utd'     # Rank 51 (ATTENTION: diff√©rent de 'Newcastle United')
]

# NOUVELLES CAT√âGORIES BAS√âES SUR LE RAPPORT TECHNIQUE
# Clubs de milieu de tableau (moyenne position 8-14)
MIDDLE_TABLE_CLUBS = [
    'Everton',
    'Aston Villa', 
    'Brighton',
    'Crystal Palace',
    'Southampton',
    'Bournemouth',
    'Leeds United',  # Note: v√©rifier le nom exact dans le dataset
    'Brentford'
]

# Clubs en lutte contre la rel√©gation (moyenne position ‚â•15)
RELEGATION_BATTLE_CLUBS = [
    'Norwich',
    'Watford',
    'Burnley',
    'Sheffield United',
    'Fulham',
    'Cardiff',
    'Huddersfield'
]

#### Step 0 ### 

def load_processed_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Charge les donn√©es nettoy√©es depuis data/processed/
    
    Returns:
    --------
    train_df, test_df : tuple de DataFrames
    """
    logger.info("Chargement des donn√©es nettoy√©es...")
    
    try:
        train_df = pd.read_csv("data/processed/train_data.csv")
        test_df = pd.read_csv("data/processed/test_data.csv")
        
        logger.info(f"Train shape: {train_df.shape}")
        logger.info(f"Test shape: {test_df.shape}")
        
        return train_df, test_df
        
    except FileNotFoundError as e:
        logger.error(f"Fichier non trouv√©: {e}")
        raise
    except Exception as e:
        logger.error(f"Erreur lors du chargement: {e}")
        raise

###### Step 1 #####  DataFrame Inspection
def inspect_dataframe(df: pd.DataFrame, name: str = "DataFrame") -> None:
    """
    Affiche un r√©sum√© informatif d'un DataFrame.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame √† inspecter
    name : str
        Nom pour l'affichage
    """
    print(f"\n{'='*60}")
    print(f"üîç INSPECTION: {name}")
    print(f"{'='*60}")
    
    print(f"Shape: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
    print(f"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    print(f"\nüìä Types de donn√©es:")
    dtype_counts = df.dtypes.value_counts()
    for dtype, count in dtype_counts.items():
        print(f"  ‚Ä¢ {dtype}: {count} colonnes")
    
    print(f"\n‚ö†Ô∏è  Valeurs manquantes:")
    missing = df.isnull().sum()
    missing_cols = missing[missing > 0]
    
    if len(missing_cols) > 0:
        for col, count in missing_cols.items():
            percentage = (count / len(df)) * 100
            print(f"  ‚Ä¢ {col}: {count} ({percentage:.1f}%)")
    else:
        print("  ‚úÖ Aucune valeur manquante")
    
    print(f"\nüéØ Variable cible (Value):")
    if 'Value' in df.columns:
        print(f"  ‚Ä¢ Min: ‚Ç¨{df['Value'].min():,.0f}")
        print(f"  ‚Ä¢ Max: ‚Ç¨{df['Value'].max():,.0f}")
        print(f"  ‚Ä¢ Mean: ‚Ç¨{df['Value'].mean():,.0f}")
        print(f"  ‚Ä¢ Skewness: {df['Value'].skew():.2f}")
    
    print(f"\nüìã 5 premi√®res lignes:")
    print(df.head())
    
    print(f"\n{'='*60}")


######### Step 2 ######## Position encoding 

def encode_position(df: pd.DataFrame) -> pd.DataFrame:
    """
    Encode la colonne Position en variables one-hot (4 cat√©gories).
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame contenant la colonne 'Position'
        
    Returns:
    --------
    df_encoded : pandas.DataFrame
        DataFrame avec les colonnes one-hot ajout√©es
    """
    logger.info("Encodage one-hot de la colonne Position...")
    
    # V√©rifier que la colonne Position existe
    if 'Position' not in df.columns:
        logger.error("Colonne 'Position' non trouv√©e")
        return df
    
    # Cr√©er une copie pour √©viter les modifications inplace
    df_encoded = df.copy()
    
    # 1. V√©rifier les valeurs uniques
    unique_positions = df_encoded['Position'].unique()
    logger.info(f"Positions uniques trouv√©es: {unique_positions}")
    
    # 2. Cr√©er les colonnes one-hot
    # Pour chaque position, cr√©er une colonne binaire
    positions_map = {
        'DF': 'is_Defender',
        'MF': 'is_Midfielder',
        'FW': 'is_Forward',
        'GK': 'is_Goalkeeper'
    }
    
    # Initialiser toutes les colonnes √† 0
    for col_name in positions_map.values():
        df_encoded[col_name] = 0
    
    # Mettre √† 1 pour la position correspondante
    for original, encoded in positions_map.items():
        mask = df_encoded['Position'] == original
        df_encoded.loc[mask, encoded] = 1
    
    # 3. V√©rifier la distribution
    logger.info("Distribution apr√®s encodage:")
    for encoded_col in positions_map.values():
        count = df_encoded[encoded_col].sum()
        percentage = (count / len(df_encoded)) * 100
        logger.info(f"  ‚Ä¢ {encoded_col}: {count} joueurs ({percentage:.1f}%)")
    
    # 4. Optionnel: Supprimer la colonne originale
    # df_encoded = df_encoded.drop('Position', axis=1)
    # Note: Pour l'instant, gardons-la pour v√©rification
    
    return df_encoded

####### Step 3 ##### Test the position encoding

def test_position_encoding():
    """
    Teste l'encodage de la position sur les donn√©es d'entra√Ænement.
    """
    print("\nüß™ TEST DE L'ENCODAGE POSITION")
    print("=" * 50)
    
    # Charger les donn√©es
    train_df, test_df = load_processed_data()
    
    # Appliquer l'encodage
    train_encoded = encode_position(train_df)
    test_encoded = encode_position(test_df)
    
    # V√©rifier les r√©sultats
    print("\n‚úÖ Encodage appliqu√© avec succ√®s!")
    print(f"\nTrain - Avant: {train_df.shape[1]} colonnes")
    print(f"Train - Apr√®s: {train_encoded.shape[1]} colonnes")
    
    print(f"\nTest - Avant: {test_df.shape[1]} colonnes")
    print(f"Test - Apr√®s: {test_encoded.shape[1]} colonnes")
    
    # Afficher les nouvelles colonnes
    new_cols = [col for col in train_encoded.columns 
                if col.startswith('is_')]
    
    print(f"\nüéØ Nouvelles colonnes cr√©√©es: {new_cols}")
    
    # V√©rifier quelques exemples
    print(f"\nüìã Exemples (premi√®res 3 lignes):")
    sample_cols = ['Player', 'Position'] + new_cols
    print(train_encoded[sample_cols].head(3))
    
    # V√©rifier la coh√©rence
    print(f"\nüîç V√©rification de coh√©rence:")
    for idx, row in train_encoded.head(5).iterrows():
        position = row['Position']
        expected_col = f"is_{position}"
        # Adapter le nom attendu
        position_map = {'DF': 'Defender', 'MF': 'Midfielder', 
                       'FW': 'Forward', 'GK': 'Goalkeeper'}
        expected_col = f"is_{position_map.get(position, position)}"
        
        if expected_col in new_cols and row[expected_col] == 1:
            print(f"  ‚úÖ Ligne {idx}: {row['Player']} - {position} ‚Üí {expected_col}=1")
        else:
            print(f"  ‚ùå Ligne {idx}: Probl√®me de coh√©rence")
    
    return train_encoded, test_encoded

######## Step 4 #### missing values treatment 

def handle_missing_values(df, is_training=True, imputation_dict=None):
    """
    G√®re les valeurs manquantes dans les colonnes num√©riques.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame d'entr√©e
    is_training : bool
        Si True, calcule les m√©dianes sur ce dataset
    imputation_dict : dict, optional
        Dictionnaire des valeurs d'imputation (m√©dianes)
        
    Returns:
    --------
    df_imputed : pandas.DataFrame
        DataFrame avec valeurs imput√©es
    imputation_dict : dict
        Dictionnaire des valeurs d'imputation utilis√©es
    """
    logger.info("Gestion des valeurs manquantes...")
    
    # Colonnes avec valeurs manquantes identifi√©es
    cols_with_missing = ['Minutes_played', 'Goals', 'Assists']
    
    # S'assurer que ces colonnes existent
    existing_cols = [col for col in cols_with_missing if col in df.columns]
    
    if not existing_cols:
        logger.info("Aucune colonne avec valeurs manquantes √† traiter")
        return df, imputation_dict or {}
    
    df_imputed = df.copy()
    
    if is_training:
        # En mode entra√Ænement : calculer les m√©dianes
        imputation_dict = {}
        for col in existing_cols:
            median_val = df_imputed[col].median()
            imputation_dict[col] = median_val
            df_imputed[col] = df_imputed[col].fillna(median_val)
            missing_count = df[col].isnull().sum()
            logger.info(f"  ‚Ä¢ {col}: {missing_count} valeurs manquantes ‚Üí imput√©es avec {median_val:.2f}")
    else:
        # En mode test : utiliser les m√©dianes du training
        if imputation_dict is None:
            raise ValueError("imputation_dict requis en mode test")
        
        for col in existing_cols:
            if col in imputation_dict:
                df_imputed[col] = df_imputed[col].fillna(imputation_dict[col])
    
    return df_imputed, imputation_dict

######## Step 5 ###### Creation of ratio

def create_ratios(df):
    """
    Cr√©e les ratios d√©riv√©s √† partir des statistiques de base.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame avec colonnes Goals, Assists, Minutes_played
        
    Returns:
    --------
    df_with_ratios : pandas.DataFrame
        DataFrame avec colonnes de ratios ajout√©es
    """
    logger.info("Cr√©ation des ratios d√©riv√©s...")
    
    df_ratios = df.copy()
    
    # 1. √âviter la division par z√©ro
    # Cr√©er une copie s√©curis√©e de Minutes_played
    minutes_safe = df_ratios['Minutes_played'].copy()
    minutes_safe[minutes_safe == 0] = 1  # Remplacer 0 par 1 pour √©viter division par z√©ro
    
    # 2. Calculer les ratios de base
    df_ratios['Goals_per_minute'] = df_ratios['Goals'] / minutes_safe
    df_ratios['Assists_per_minute'] = df_ratios['Assists'] / minutes_safe
    
    # 3. Appliquer transformation log pour r√©duire l'asym√©trie
    df_ratios['Goals_per_minute_log'] = np.log1p(df_ratios['Goals_per_minute'])
    df_ratios['Assists_per_minute_log'] = np.log1p(df_ratios['Assists_per_minute'])
    
    # 4. Statistiques sur les ratios cr√©√©s
    logger.info("Ratios cr√©√©s avec succ√®s:")
    for ratio_col in ['Goals_per_minute', 'Assists_per_minute']:
        if ratio_col in df_ratios.columns:
            logger.info(f"  ‚Ä¢ {ratio_col}: min={df_ratios[ratio_col].min():.6f}, "
                       f"max={df_ratios[ratio_col].max():.6f}, "
                       f"mean={df_ratios[ratio_col].mean():.6f}")
    
    return df_ratios



####### Step 6 ###### Test of ratio 

def test_ratios_and_imputation():
    """
    Teste la cr√©ation des ratios et l'imputation des valeurs manquantes.
    """
    print("\nüß™ TEST DES RATIOS ET IMPUTATION")
    print("=" * 50)
    
    # Charger les donn√©es
    train_df, test_df = load_processed_data()
    
    # √âtape 1: Encodage position (d√©j√† test√©)
    print("\n1. Encodage de la position...")
    train_encoded = encode_position(train_df)
    test_encoded = encode_position(test_df)
    
    # √âtape 2: Gestion des valeurs manquantes
    print("\n2. Imputation des valeurs manquantes...")
    train_imputed, imputation_dict = handle_missing_values(train_encoded, is_training=True)
    test_imputed, _ = handle_missing_values(test_encoded, is_training=False, imputation_dict=imputation_dict)
    
    # V√©rifier qu'il n'y a plus de valeurs manquantes
    missing_train = train_imputed[['Minutes_played', 'Goals', 'Assists']].isnull().sum().sum()
    missing_test = test_imputed[['Minutes_played', 'Goals', 'Assists']].isnull().sum().sum()
    
    print(f"   ‚úÖ Train - Valeurs manquantes restantes: {missing_train}")
    print(f"   ‚úÖ Test - Valeurs manquantes restantes: {missing_test}")
    print(f"   üìä Valeurs d'imputation utilis√©es: {imputation_dict}")
    
    # √âtape 3: Cr√©ation des ratios
    print("\n3. Cr√©ation des ratios...")
    train_with_ratios = create_ratios(train_imputed)
    test_with_ratios = create_ratios(test_imputed)
    
    # V√©rification
    print(f"\n‚úÖ Toutes les √©tapes appliqu√©es avec succ√®s!")
    print(f"\nüìä Dimensions des datasets:")
    print(f"   Train: {train_with_ratios.shape[0]} lignes √ó {train_with_ratios.shape[1]} colonnes")
    print(f"   Test: {test_with_ratios.shape[0]} lignes √ó {test_with_ratios.shape[1]} colonnes")
    
    # Afficher les nouvelles colonnes
    new_cols = [col for col in train_with_ratios.columns 
                if col not in train_df.columns and not col.startswith('is_')]
    print(f"\nüéØ Nouvelles colonnes cr√©√©es:")
    for col in new_cols:
        print(f"   ‚Ä¢ {col}")
    
    # Aper√ßu des ratios
    print(f"\nüìã Exemples de ratios (premi√®res 3 lignes):")
    ratio_cols = ['Goals', 'Minutes_played', 'Goals_per_minute', 'Goals_per_minute_log']
    print(train_with_ratios[ratio_cols].head(3))
    
    # Statistiques des ratios
    print(f"\nüìà Statistiques des ratios (train set):")
    for ratio in ['Goals_per_minute', 'Assists_per_minute']:
        if ratio in train_with_ratios.columns:
            data = train_with_ratios[ratio]
            print(f"   ‚Ä¢ {ratio}:")
            print(f"      Min: {data.min():.6f}")
            print(f"      Max: {data.max():.6f}")
            print(f"      Moyenne: {data.mean():.6f}")
            print(f"      M√©diane: {data.median():.6f}")
    
    return train_with_ratios, test_with_ratios, imputation_dict

###### Step 7 ###### club encoding using ALL 4 categories

def encode_clubs(df, is_training=True, top_clubs=None, middle_clubs=None, relegation_clubs=None):
    """
    Encode les clubs en utilisant les 4 cat√©gories : Top, Middle-table, Relegation-battle, Other.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame avec colonne 'Club'
    is_training : bool
        Si True, utilise les listes pr√©d√©finies
        Si False, utilise les listes fournies en param√®tre
    top_clubs : list, optional
        Liste des clubs Top pour le test set
    middle_clubs : list, optional
        Liste des clubs Middle-table pour le test set (nouveau)
    relegation_clubs : list, optional
        Liste des clubs Relegation-battle pour le test set (nouveau)
        
    Returns:
    --------
    df_encoded : pandas.DataFrame
        DataFrame avec encoding des clubs
    club_metadata : dict
        Dictionnaire contenant les listes de clubs utilis√©es
    """
    logger.info("Encodage des clubs avec 4 cat√©gories...")
    
    if 'Club' not in df.columns:
        logger.warning("Colonne 'Club' non trouv√©e")
        # Retourner des m√©tadonn√©es vides pour compatibilit√©
        if is_training:
            return df, {'top_clubs': TOP_CLUBS_EURO_RANKING,
                       'middle_clubs': MIDDLE_TABLE_CLUBS,
                       'relegation_clubs': RELEGATION_BATTLE_CLUBS}
        else:
            return df, {'top_clubs': top_clubs or [],
                       'middle_clubs': middle_clubs or [],
                       'relegation_clubs': relegation_clubs or []}
    
    df_encoded = df.copy()
    
    # D√©terminer les listes de clubs √† utiliser
    if is_training:
        # En mode entra√Ænement : utiliser nos listes pr√©d√©finies
        top_clubs_used = TOP_CLUBS_EURO_RANKING
        middle_clubs_used = MIDDLE_TABLE_CLUBS
        relegation_clubs_used = RELEGATION_BATTLE_CLUBS
        logger.info("Mode entra√Ænement : utilisation des listes pr√©d√©finies")
    else:
        # En mode test : utiliser les listes pass√©es en param√®tre
        # Si certaines listes ne sont pas fournies, utiliser des listes vides
        top_clubs_used = top_clubs or []
        middle_clubs_used = middle_clubs or []
        relegation_clubs_used = relegation_clubs or []
        logger.info("Mode test : utilisation des listes fournies")
    
    # V√©rifier quels clubs de la liste sont pr√©sents (pour le logging)
    def check_presence(club_list, category_name):
        present = [club for club in club_list if club in df_encoded['Club'].values]
        missing = [club for club in club_list if club not in df_encoded['Club'].values]
        logger.info(f"{category_name}: {len(present)}/{len(club_list)} pr√©sents")
        if missing:
            logger.debug(f"  Absents: {missing[:5]}{'...' if len(missing) > 5 else ''}")
    
    check_presence(top_clubs_used, "Top clubs")
    check_presence(middle_clubs_used, "Middle-table clubs")
    check_presence(relegation_clubs_used, "Relegation-battle clubs")
    
    # Cr√©er la fonction de cat√©gorisation
    def categorize_club(club_name):
        if club_name in top_clubs_used:
            return 'Top_Club'
        elif club_name in middle_clubs_used:
            return 'Middle_Table_Club'
        elif club_name in relegation_clubs_used:
            return 'Relegation_Battle_Club'
        else:
            return 'Other_Club'
    
    # Appliquer la cat√©gorisation
    df_encoded['Club_encoded'] = df_encoded['Club'].apply(categorize_club)
    
    # Statistiques sur la r√©partition
    logger.info("R√©partition apr√®s cat√©gorisation:")
    categories = ['Top_Club', 'Middle_Table_Club', 'Relegation_Battle_Club', 'Other_Club']
    for category in categories:
        count = (df_encoded['Club_encoded'] == category).sum()
        percentage = (count / len(df_encoded)) * 100
        logger.info(f"  ‚Ä¢ {category}: {count} joueurs ({percentage:.1f}%)")
    
    # Cr√©er les variables dummies (one-hot encoding)
    club_dummies = pd.get_dummies(df_encoded['Club_encoded'], prefix='club')
    
    # Ajouter les dummies au dataframe principal
    df_encoded = pd.concat([df_encoded, club_dummies], axis=1)
    
    logger.info(f"Encodage termin√©. {len(club_dummies.columns)} colonnes club cr√©√©es.")
    
    # Afficher les colonnes cr√©√©es
    if len(club_dummies.columns) <= 10:
        logger.info(f"Colonnes cr√©√©es: {list(club_dummies.columns)}")
    
    # Pr√©parer les m√©tadonn√©es pour reproduction
    club_metadata = {
        'top_clubs': top_clubs_used,
        'middle_clubs': middle_clubs_used,
        'relegation_clubs': relegation_clubs_used,
        'club_categories': categories
    }
    
    return df_encoded, club_metadata

###### Step 8 ##### Preparation for the log transformation 

def prepare_target(df):
    """
    Pr√©pare la variable cible avec transformation log.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame avec colonne 'Value'
        
    Returns:
    --------
    y_log : pandas.Series
        Variable cible transform√©e (log(1 + Value))
    y_original : pandas.Series
        Variable cible originale (pour r√©f√©rence)
    """
    logger.info("Pr√©paration de la variable cible...")
    
    if 'Value' not in df.columns:
        raise ValueError("Colonne 'Value' non trouv√©e")
    
    y_original = df['Value'].copy()
    
    # Transformation log pour g√©rer l'asym√©trie
    y_log = np.log1p(y_original)
    
    logger.info(f"Transformation: log(1 + Value)")
    logger.info(f"Original - Min: ‚Ç¨{y_original.min():,.0f}, Max: ‚Ç¨{y_original.max():,.0f}")
    logger.info(f"Log - Min: {y_log.min():.2f}, Max: {y_log.max():.2f}")
    logger.info(f"Skewness original: {y_original.skew():.2f}")
    logger.info(f"Skewness log: {y_log.skew():.2f}")
    
    return y_log, y_original

##### Step 9 ##### final code for the doc (pipeline)

def create_final_features(df, is_training=True, top_clubs=None, imputation_dict=None):
    """
    Pipeline complet de feature engineering.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Donn√©es d'entr√©e
    is_training : bool
        Si True, entra√Æne les transformateurs
    top_clubs : list, optional
        Liste des top clubs (maintenant obsol√®te, gard√© pour compatibilit√©)
    imputation_dict : dict, optional
        Dictionnaire d'imputation
        
    Returns:
    --------
    X : pandas.DataFrame
        Features finales
    y_log : pandas.Series
        Target transform√©e
    metadata : dict
        M√©tadonn√©es (top_clubs, imputation_dict, etc.)
    """
    logger.info(f"Pipeline de feature engineering (is_training={is_training})")
    
    # Pour la compatibilit√©: si top_clubs est fourni mais est une liste simple,
    # c'est probablement l'ancien format. Dans ce cas, on l'ignore car
    # encode_clubs s'attend maintenant √† 3 listes s√©par√©es.
    
    # 1. Encodage position
    df_encoded = encode_position(df)
    
    # 2. Imputation valeurs manquantes
    df_imputed, imputation_dict = handle_missing_values(
        df_encoded, is_training=is_training, imputation_dict=imputation_dict
    )
    
    # 3. Cr√©ation ratios
    df_ratios = create_ratios(df_imputed)
    
    # 4. Encodage clubs - maintenant avec 4 cat√©gories
    # Pour la compatibilit√© ascendante, on passe top_clubs comme premier argument
    # mais encode_clubs s'attend √† 3 listes s√©par√©es
    df_clubs, club_metadata = encode_clubs(
        df_ratios, 
        is_training=is_training,
        top_clubs=top_clubs,  # Conserve pour compatibilit√©
        middle_clubs=None,     # Seront d√©termin√©s automatiquement si is_training=True
        relegation_clubs=None  # Seront d√©termin√©s automatiquement si is_training=True
    )
    
    # 5. S√©lection des features pour le mod√®le
    # Garder les features num√©riques de base
    numerical_features = [
        'Age', 'Minutes_played', 'Goals', 'Assists',
        'Height', 'Weight', 'Matchs_played',
        'Goals_per_minute_log', 'Assists_per_minute_log'
    ]
    
    # Features de position (one-hot)
    position_features = [col for col in df_clubs.columns if col.startswith('is_')]
    
    # Features de club (one-hot) 
    club_features = [col for col in df_clubs.columns if col.startswith('club_')]
    
    # Nation (on garde comme cat√©gorielle pour l'instant)
    if 'Nation' in df_clubs.columns:
        # Pour les mod√®les d'arbre, on peut encoder en num√©rique
        df_clubs['Nation_encoded'] = pd.factorize(df_clubs['Nation'])[0]
        numerical_features.append('Nation_encoded')
    
    # Combiner toutes les features
    all_features = numerical_features + position_features + club_features
    
    # Garder seulement les colonnes qui existent
    existing_features = [col for col in all_features if col in df_clubs.columns]
    
    X = df_clubs[existing_features].copy()
    
    # 6. Pr√©paration de la target
    y_log, y_original = prepare_target(df_clubs)
    
    # Metadata pour reproduction
    metadata = {
        'top_clubs': club_metadata.get('top_clubs', []),
        'middle_clubs': club_metadata.get('middle_clubs', []),
        'relegation_clubs': club_metadata.get('relegation_clubs', []),
        'club_categories': club_metadata.get('club_categories', []),
        'imputation_dict': imputation_dict,
        'feature_names': existing_features,
        'n_features': len(existing_features),
        'n_numerical': len([f for f in existing_features if f in numerical_features]),
        'n_position': len([f for f in existing_features if f in position_features]),
        'n_club': len([f for f in existing_features if f in club_features])
    }
    
    logger.info(f"Features finales: {len(existing_features)} colonnes")
    logger.info(f"  ‚Ä¢ Num√©riques: {metadata['n_numerical']}")
    logger.info(f"  ‚Ä¢ Position: {metadata['n_position']}")
    logger.info(f"  ‚Ä¢ Club (4 cat√©gories): {metadata['n_club']}")
    
    return X, y_log, metadata

##### Step 10 #### Test of the final Pipeline

def test_complete_pipeline():
    """
    Teste le pipeline complet de feature engineering.
    """
    print("\nüß™ TEST COMPLET DU PIPELINE DE FEATURE ENGINEERING")
    print("=" * 60)
    
    # Charger les donn√©es
    train_df, test_df = load_processed_data()
    
    print("\n1. Application sur le train set...")
    X_train, y_train_log, metadata = create_final_features(train_df, is_training=True)
    
    print(f"\n‚úÖ Train set transform√©:")
    print(f"   ‚Ä¢ X shape: {X_train.shape}")
    print(f"   ‚Ä¢ y shape: {y_train_log.shape}")
    print(f"   ‚Ä¢ Nombre de features: {metadata['n_features']}")
    print(f"   ‚Ä¢ Cat√©gories club: {metadata['club_categories']}")
    
    print("\n2. Application sur le test set (avec m√©tadonn√©es du train)...")
    # Extraire les listes de clubs des m√©tadonn√©es
    X_test, y_test_log, _ = create_final_features(
        test_df, 
        is_training=False,
        top_clubs=metadata['top_clubs'],  # Pass√© pour compatibilit√©
        imputation_dict=metadata['imputation_dict']
    )
    
    print(f"\n‚úÖ Test set transform√©:")
    print(f"   ‚Ä¢ X shape: {X_test.shape}")
    print(f"   ‚Ä¢ y shape: {y_test_log.shape}")
    
    # V√©rifier la coh√©rence des colonnes
    print(f"\n3. V√©rification de coh√©rence...")
    train_cols = set(X_train.columns)
    test_cols = set(X_test.columns)
    
    if train_cols == test_cols:
        print(f"   ‚úÖ M√™mes colonnes dans train et test")
    else:
        missing_in_test = train_cols - test_cols
        missing_in_train = test_cols - train_cols
        if missing_in_test:
            print(f"   ‚ö†Ô∏è  Colonnes manquantes dans test: {missing_in_test}")
        if missing_in_train:
            print(f"   ‚ö†Ô∏è  Colonnes manquantes dans train: {missing_in_train}")
    
    # Afficher un √©chantillon des features
    print(f"\n4. √âchantillon des features (5 premi√®res):")
    print(f"   ‚Ä¢ Train: {list(X_train.columns)[:5]}")
    print(f"   ‚Ä¢ Test: {list(X_test.columns)[:5]}")
    
    # Statistiques des features
    print(f"\n5. Types de features:")
    feature_types = {
        'Num√©rique': [col for col in X_train.columns 
                     if not col.startswith('is_') and not col.startswith('club_')],
        'Position': [col for col in X_train.columns if col.startswith('is_')],
        'Club': [col for col in X_train.columns if col.startswith('club_')]
    }
    
    for type_name, features in feature_types.items():
        if features:
            print(f"   ‚Ä¢ {type_name}: {len(features)} features")
            if len(features) <= 5:  # Afficher si peu de features
                print(f"     {features}")
    
    return X_train, y_train_log, X_test, y_test_log, metadata

##### Step 11 #### function that save the transformed data

def save_processed_data(X_train, y_train, X_test, y_test, metadata, output_dir="data/processed"):
    """
    Sauvegarde les donn√©es transform√©es et les m√©tadonn√©es.
    
    Parameters:
    -----------
    X_train, X_test : pandas.DataFrame
        Features d'entra√Ænement et de test
    y_train, y_test : pandas.Series
        Target transform√©e (log scale)
    metadata : dict
        M√©tadonn√©es du pipeline
    output_dir : str
        Dossier de sauvegarde
    """
    import os
    import json
    
    logger.info(f"Sauvegarde des donn√©es transform√©es dans {output_dir}...")
    
    # Cr√©er le dossier s'il n'existe pas
    os.makedirs(output_dir, exist_ok=True)
    
    # Sauvegarder les DataFrames
    X_train.to_csv(f"{output_dir}/X_train_transformed.csv", index=False)
    X_test.to_csv(f"{output_dir}/X_test_transformed.csv", index=False)
    
    # Sauvegarder les targets (en log scale)
    y_train.to_csv(f"{output_dir}/y_train_log.csv", index=False, header=['Value_log'])
    y_test.to_csv(f"{output_dir}/y_test_log.csv", index=False, header=['Value_log'])
    
    # Sauvegarder les m√©tadonn√©es au format JSON
    metadata_serializable = {
        'top_clubs': metadata['top_clubs'],
        'middle_clubs': metadata['middle_clubs'],
        'relegation_clubs': metadata['relegation_clubs'],
        'club_categories': metadata['club_categories'],
        'imputation_dict': {k: float(v) for k, v in metadata['imputation_dict'].items()},
        'feature_names': metadata['feature_names'],
        'n_features': metadata['n_features'],
        'timestamp': pd.Timestamp.now().isoformat()
    }
    
    with open(f"{output_dir}/feature_metadata.json", 'w') as f:
        json.dump(metadata_serializable, f, indent=2)
    
    # Sauvegarder aussi les targets originales (pour r√©f√©rence)
    train_df, test_df = load_processed_data()
    train_df['Value'].to_csv(f"{output_dir}/y_train_original.csv", index=False, header=['Value'])
    test_df['Value'].to_csv(f"{output_dir}/y_test_original.csv", index=False, header=['Value'])
    
    logger.info("‚úÖ Donn√©es sauvegard√©es avec succ√®s:")
    logger.info(f"   ‚Ä¢ X_train: {output_dir}/X_train_transformed.csv ({X_train.shape})")
    logger.info(f"   ‚Ä¢ X_test: {output_dir}/X_test_transformed.csv ({X_test.shape})")
    logger.info(f"   ‚Ä¢ y_train_log: {output_dir}/y_train_log.csv")
    logger.info(f"   ‚Ä¢ y_test_log: {output_dir}/y_test_log.csv")
    logger.info(f"   ‚Ä¢ M√©tadonn√©es: {output_dir}/feature_metadata.json")
    
    return output_dir

##### Step 12 #### Function that execute he pipeline and save the steps 

def run_and_save_pipeline():
    """
    Ex√©cute le pipeline complet et sauvegarde les r√©sultats.
    """
    print("üöÄ EX√âCUTION ET SAUVEGARDE DU PIPELINE COMPLET")
    print("=" * 60)
    
    # 1. Charger les donn√©es
    train_df, test_df = load_processed_data()
    
    # 2. Ex√©cuter le pipeline sur le train set
    print("\n1. Feature engineering sur le train set...")
    X_train, y_train_log, metadata = create_final_features(train_df, is_training=True)
    
    # 3. Ex√©cuter sur le test set avec les m√©tadonn√©es du train
    print("\n2. Feature engineering sur le test set...")
    X_test, y_test_log, _ = create_final_features(
        test_df, 
        is_training=False,
        top_clubs=metadata['top_clubs'],
        imputation_dict=metadata['imputation_dict']
    )
    
    # 4. Sauvegarder
    print("\n3. Sauvegarde des donn√©es transform√©es...")
    save_path = save_processed_data(X_train, y_train_log, X_test, y_test_log, metadata)
    
    # 5. R√©sum√©
    print("\n" + "=" * 60)
    print("‚úÖ PIPELINE TERMIN√â ET DONN√âES SAUVEGARD√âES")
    print("=" * 60)
    
    summary = f"""
üìä R√âSUM√â FINAL PHASE 4:
   ‚Ä¢ Features cr√©√©es: {metadata['n_features']}
   ‚Ä¢ Train samples: {X_train.shape[0]}
   ‚Ä¢ Test samples: {X_test.shape[0]}
   ‚Ä¢ Cat√©gories de clubs: {len(metadata['club_categories'])}
   ‚Ä¢ Donn√©es sauvegard√©es dans: {save_path}

üîß TYPES DE FEATURES:
   ‚Ä¢ Num√©riques: {metadata['n_numerical']}
   ‚Ä¢ Position: {metadata['n_position']}
   ‚Ä¢ Club: {metadata['n_club']}

üéØ NOUVELLES CAT√âGORIES DE CLUBS:
   ‚Ä¢ Top clubs: {len(metadata['top_clubs'])} clubs
   ‚Ä¢ Middle-table clubs: {len(metadata['middle_clubs'])} clubs
   ‚Ä¢ Relegation-battle clubs: {len(metadata['relegation_clubs'])} clubs
   ‚Ä¢ Other clubs: cat√©gorie r√©siduelle

üéØ PR√äT POUR LA PHASE 5 (MOD√âLISATION)!
    """
    print(summary)
    
    return X_train, y_train_log, X_test, y_test_log, metadata

#### Final Test ‚Äú######

if __name__ == "__main__":
    """
    Point d'entr√©e pour tester et sauvegarder.
    """
    print("üß™ MODULE FEATURE ENGINEERING - EX√âCUTION COMPL√àTE")
    
    try:
        # Option 1: Ex√©cuter et sauvegarder le pipeline complet
        X_train, y_train_log, X_test, y_test_log, metadata = run_and_save_pipeline()
        
        # Option 2: Tester le chargement des donn√©es sauvegard√©es
        print("\n" + "="*60)
        print("üîç TEST DE CHARGEMENT DES DONN√âES SAUVEGARD√âES")
        print("="*60)
        
        # Cr√©er une fonction de test rapide
        test_df = pd.read_csv("data/processed/X_train_transformed.csv")
        print(f"‚úÖ Donn√©es charg√©es: {test_df.shape}")
        print(f"   Colonnes: {list(test_df.columns)[:5]}...")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
