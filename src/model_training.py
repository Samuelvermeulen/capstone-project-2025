
"""
Phase 5 - Model Training Module (Step-by-Step)
"""

# ============================================================================
# 1. CORE PYTHON LIBRARIES
# ============================================================================
import pandas as pd  # Data manipulation
import numpy as np   # Numerical operations
import json          # For reading/writing metadata
import logging       # For tracking execution
import os            # For file operations
from typing import Dict, Tuple  # Type hints for better code clarity
from datetime import datetime   # For timestamping results

# ============================================================================
# 2. MACHINE LEARNING LIBRARIES
# ============================================================================
# Scikit-learn models
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit

# XGBoost 
try:
    from xgboost import XGBRegressor
    XGB_AVAILABLE = True
except ImportError:
    XGB_AVAILABLE = False
    # We'll handle this gracefully - the model won't be available but others will work

# ============================================================================
# 3. LOGGING CONFIGURATION
# ============================================================================
# Configure how messages are displayed during execution
logging.basicConfig(
    level=logging.INFO,  # Show INFO level messages and above
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)  # Create logger for this module

print("‚úÖ Part 1 loaded: Imports and configuration")

# ============================================================================
# 4. DATA LOADING FUNCTION
# ============================================================================
def load_transformed_data(data_dir: str = "data/processed") -> Tuple:
    """
    Load the 25 engineered features and transformed targets from Phase 4.
    
    This function loads ALL the data prepared in the previous phase:
    - Transformed features (X_train, X_test) with 25 columns each
    - Targets in log scale (log transformation from Phase 4)
    - Original targets in euros (for final evaluation)
    - Metadata about how features were engineered
    
    Returns:
        Tuple with 7 elements:
        X_train, X_test: Features for training and testing
        y_train_log, y_test_log: Targets in log scale (what models will predict)
        y_train_orig, y_test_orig: Original values in euros (for interpretation)
        metadata: Information about feature engineering process
    """
    logger.info(f"üì• Loading transformed data from {data_dir}...")
    
    try:
        # --------------------------------------------------------------------
        # 4.1 LOAD TRANSFORMED FEATURES (25 features from Phase 4)
        # --------------------------------------------------------------------
        X_train = pd.read_csv(f"{data_dir}/X_train_transformed.csv")
        X_test = pd.read_csv(f"{data_dir}/X_test_transformed.csv")
        
        # --------------------------------------------------------------------
        # 4.2 LOAD TARGETS IN LOG SCALE (what models will predict)
        # --------------------------------------------------------------------
        # Note: We use log transformation because player values have huge range
        y_train_log = pd.read_csv(f"{data_dir}/y_train_log.csv")['Value_log']
        y_test_log = pd.read_csv(f"{data_dir}/y_test_log.csv")['Value_log']
        
        # --------------------------------------------------------------------
        # 4.3 LOAD ORIGINAL TARGETS IN EUROS (for final metrics)
        # --------------------------------------------------------------------
        # These are the actual market values in euros
        y_train_orig = pd.read_csv(f"{data_dir}/y_train_original.csv")['Value']
        y_test_orig = pd.read_csv(f"{data_dir}/y_test_original.csv")['Value']
        
        # --------------------------------------------------------------------
        # 4.4 LOAD METADATA (information about feature engineering)
        # --------------------------------------------------------------------
        with open(f"{data_dir}/feature_metadata.json", 'r') as f:
            metadata = json.load(f)
        
        # --------------------------------------------------------------------
        # 4.5 LOG SUCCESS MESSAGE WITH KEY STATISTICS
        # --------------------------------------------------------------------
        logger.info("‚úÖ Data loaded successfully:")
        logger.info(f"   ‚Ä¢ Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features")
        logger.info(f"   ‚Ä¢ Test set: {X_test.shape[0]} samples")
        logger.info(f"   ‚Ä¢ Features: {list(X_train.columns)[:5]}...")  # Show first 5 features
        logger.info(f"   ‚Ä¢ Target range (original): ‚Ç¨{y_train_orig.min():,.0f} to ‚Ç¨{y_train_orig.max():,.0f}")
        
        return X_train, X_test, y_train_log, y_test_log, y_train_orig, y_test_orig, metadata
        
    except FileNotFoundError as e:
        logger.error(f"‚ùå File not found: {e}")
        logger.error("Please run Phase 4 (feature engineering) first")
        logger.error("Expected files in data/processed/:")
        logger.error("  - X_train_transformed.csv")
        logger.error("  - X_test_transformed.csv")
        logger.error("  - y_train_log.csv, y_test_log.csv")
        logger.error("  - y_train_original.csv, y_test_original.csv")
        logger.error("  - feature_metadata.json")
        raise
    except Exception as e:
        logger.error(f"‚ùå Error during loading: {e}")
        raise

print("‚úÖ Part 2 loaded: Data loading function")


# ============================================================================
# 5. MODEL INITIALIZATION FUNCTION
# ============================================================================
def initialize_models(seed: int = 42) -> Dict:
    """
    Initialize the 4 machine learning models according to project roadmap.
    
    Models and their parameters (based on roadmap specifications):
    1. Linear Regression: Simple baseline linear model
    2. Ridge Regression: Linear model with L2 regularization (alpha=1.0)
    3. Random Forest: Ensemble of 100 decision trees (max depth 10)
    4. XGBoost: Advanced gradient boosting (100 trees, learning_rate=0.1)
    
    Args:
        seed: Random seed for reproducibility (42 is standard in ML)
    
    Returns:
        Dictionary where keys are model names and values are model instances
    """
    logger.info("ü§ñ Initializing machine learning models...")
    
    # ------------------------------------------------------------------------
    # 5.1 CREATE MODELS DICTIONARY
    # ------------------------------------------------------------------------
    models = {
        'Linear Regression': LinearRegression(),
        
        'Ridge Regression': Ridge(
            alpha=1.0,           # Regularization strength (from roadmap)
            random_state=seed    # For reproducibility
        ),
        
        'Random Forest': RandomForestRegressor(
            n_estimators=100,    # 100 trees (from roadmap)
            max_depth=10,        # Limit tree depth (from roadmap)
            random_state=seed,   # For reproducibility
            n_jobs=-1,           # Use all CPU cores for faster training
            verbose=0            # No training messages
        )
    }
    
    # ------------------------------------------------------------------------
    # 5.2 ADD XGBOOST IF AVAILABLE 
    # ------------------------------------------------------------------------
    if XGB_AVAILABLE:
        models['XGBoost'] = XGBRegressor(
            n_estimators=100,      # 100 trees (from roadmap)
            learning_rate=0.1,     # Step size shrinkage (from roadmap)
            max_depth=5,           # Maximum tree depth (from roadmap)
            random_state=seed,     # For reproducibility
            n_jobs=-1,             # Use all CPU cores
             # Implicit parameters (XGBoost >=1.7 defaults):
    # - objective='reg:squarederror'
    # - eval_metric='rmse'
            verbosity=0            # No training messages
        )
        logger.info("‚úÖ XGBoost available and initialized")
    else:
        logger.warning("‚ö†Ô∏è  XGBoost not available")
        logger.warning("   To install: pip install xgboost")
        logger.warning("   Continuing with 3 models instead of 4")
    
    # ------------------------------------------------------------------------
    # 5.3 LOG INITIALIZATION SUCCESS
    # ------------------------------------------------------------------------
    logger.info(f"‚úÖ {len(models)} models initialized")
    logger.info("   Models available:")
    for name in models.keys():
        logger.info(f"     ‚Ä¢ {name}")
    
    return models

print("‚úÖ Part 3 loaded: Model initialization function")

# ============================================================================
# 6. TESTING FUNCTION FOR STEP 5.1
# ============================================================================
def test_step_5_1() -> bool:
    """
    Test function to validate Step 5.1 works correctly.
    
    This function:
    1. Loads the transformed data
    2. Initializes the models
    3. Displays key information for verification
    
    Returns:
        True if successful, False if any error occurs
    """
    print("=" * 70)
    print("üß™ TESTING STEP 5.1 - DATA LOADING & MODEL INITIALIZATION")
    print("=" * 70)
    
    try:
        # --------------------------------------------------------------------
        # 6.1 TEST DATA LOADING
        # --------------------------------------------------------------------
        print("\n1. üì• Testing data loading function...")
        X_train, X_test, y_train_log, y_test_log, y_train_orig, y_test_orig, metadata = load_transformed_data()
        
        print(f"   ‚úì Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features")
        print(f"   ‚úì Test set: {X_test.shape[0]} samples")
        print(f"   ‚úì Original value range: ‚Ç¨{y_train_orig.min():,.0f} to ‚Ç¨{y_train_orig.max():,.0f}")
        
        # Show feature names (first 10)
        features = metadata.get('feature_names', [])
        print(f"   ‚úì Features ({len(features)} total): {features[:10]}")
        if len(features) > 10:
            print(f"     ... and {len(features) - 10} more")
        
        # --------------------------------------------------------------------
        # 6.2 TEST MODEL INITIALIZATION
        # --------------------------------------------------------------------
        print("\n2. ü§ñ Testing model initialization...")
        models = initialize_models(seed=42)
        
        print(f"   ‚úì Models initialized: {len(models)}")
        for name, model in models.items():
            print(f"     ‚Ä¢ {name}: {type(model).__name__}")
        
        # --------------------------------------------------------------------
        # 6.3 DISPLAY SAMPLE OF TRAINING DATA
        # --------------------------------------------------------------------
        print("\n3. üîç Sample of training data (first 3 rows, first 5 columns):")
        print(X_train.iloc[:3, :5].to_string())
        
        # --------------------------------------------------------------------
        # 6.4 SUCCESS MESSAGE
        # --------------------------------------------------------------------
        print("\n" + "=" * 70)
        print("‚úÖ STEP 5.1 VALIDATED SUCCESSFULLY!")
        print("=" * 70)
        print("\nNext steps:")
        print("  1. Model training (coming in Step 5.2)")
        print("  2. Model evaluation")
        print("  3. Comparison with baseline")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå ERROR IN STEP 5.1: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# 7. MAIN EXECUTION BLOCK
# ============================================================================
if __name__ == "__main__":
    """
    This block runs when you execute the script directly.
    It's the entry point for testing Step 5.1.
    """
    test_step_5_1()

    # ============================================================================
# 8. MODEL TRAINING FUNCTIONS (STEP 5.2)
# ============================================================================

# We need to import these for Step 5.2
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

### correct the dynamic impact of premier league
def align_features(X_train, X_test):
    """
    Align features between training and test sets.
    
    IMPORTANT: In time-series splits, features can differ between periods.
    Example: Leicester City in training (2018-2022) but not in test (2022-2023).
    
    Returns:
        X_train_aligned, X_test_aligned: DataFrames with same columns
        missing_columns_train: Columns in test but not in train
        missing_columns_test: Columns in train but not in test
    """
    logger.info("üîÑ Aligning features between training and test sets...")
    
    # Get columns from both sets
    train_cols = set(X_train.columns)
    test_cols = set(X_test.columns)
    
    # Find differences
    missing_in_test = train_cols - test_cols  # In train but not test
    missing_in_train = test_cols - train_cols  # In test but not train
    
    if missing_in_test:
        logger.warning(f"‚ö†Ô∏è  Columns in training but missing in test: {missing_in_test}")
        logger.warning("  This happens when clubs/features exist in earlier seasons but not later.")
    
    if missing_in_train:
        logger.warning(f"‚ö†Ô∏è  Columns in test but missing in training: {missing_in_train}")
        logger.warning("  This happens when new clubs/features appear in later seasons.")
    
    # Create aligned DataFrames
    all_columns = sorted(train_cols.union(test_cols))
    
    X_train_aligned = pd.DataFrame(0, index=X_train.index, columns=all_columns)
    X_test_aligned = pd.DataFrame(0, index=X_test.index, columns=all_columns)
    
    # Copy existing values
    for col in all_columns:
        if col in X_train.columns:
            X_train_aligned[col] = X_train[col]
        if col in X_test.columns:
            X_test_aligned[col] = X_test[col]
    
    logger.info(f"‚úÖ Features aligned. Final shape:")
    logger.info(f"   ‚Ä¢ Training: {X_train_aligned.shape}")
    logger.info(f"   ‚Ä¢ Test: {X_test_aligned.shape}")
    logger.info(f"   ‚Ä¢ Total features: {len(all_columns)}")
    
    return X_train_aligned, X_test_aligned, missing_in_test, missing_in_train


def train_models(models_dict: Dict, X_train: pd.DataFrame, y_train_log: pd.Series) -> Tuple:
    """
    UPDATED: Now stores scaler with each model to fix feature alignment
    
    Train all 4 models on the training data.
    
    IMPORTANT: We use different preprocessing for different model types:
    - Linear models (Linear Regression, Ridge): Standardized features
    - Tree-based models (Random Forest, XGBoost): Original features
    
    Args:
        models_dict: Dictionary of models from initialize_models()
        X_train: Training features (aligned, 25+ columns)
        y_train_log: Training target in log scale
    
    Returns:
        Tuple: (trained_models, training_metrics)
        - trained_models: Dictionary with model and scaler info
        - training_metrics: Dictionary with training performance
    """
    logger.info("üéØ Starting model training...")
    
    trained_models = {}
    training_metrics = {}
    
    # ------------------------------------------------------------------------
    # 8.1 PREPARE DATA FOR DIFFERENT MODEL TYPES
    # ------------------------------------------------------------------------
    # We'll handle scaling separately for each model type
    # Linear models: need their own scaler (fit on aligned data)
    # Tree models: use data as-is
    
    # Prepare scaled data for linear models
    scaler_linear = StandardScaler()
    X_train_scaled = pd.DataFrame(
        scaler_linear.fit_transform(X_train),
        columns=X_train.columns,
        index=X_train.index
    )
    
    logger.info(f"Data prepared for training:")
    logger.info(f"  ‚Ä¢ Original shape: {X_train.shape}")
    logger.info(f"  ‚Ä¢ Scaled shape (for linear models): {X_train_scaled.shape}")
    
    # ------------------------------------------------------------------------
    # 8.2 TRAIN EACH MODEL WITH APPROPRIATE PREPROCESSING
    # ------------------------------------------------------------------------
    for model_name, model in models_dict.items():
        logger.info(f"  Training {model_name}...")
        
        start_time = datetime.now()
        
        if model_name in ['Linear Regression', 'Ridge Regression']:
            # -----------------------------------------------------------------
            # 8.2.1 LINEAR MODELS: Use standardized features
            # -----------------------------------------------------------------
            logger.debug(f"    Using standardized features for {model_name}")
            model.fit(X_train_scaled, y_train_log)
            
            # Store model with its scaler
            trained_models[model_name] = {
                'model': model,
                'scaler': scaler_linear,  # Same scaler for both linear models
                'requires_scaling': True
            }
            
            X_train_for_pred = X_train_scaled
            
        else:
            # -----------------------------------------------------------------
            # 8.2.2 TREE-BASED MODELS: Use original features
            # -----------------------------------------------------------------
            logger.debug(f"    Using original features for {model_name}")
            model.fit(X_train, y_train_log)
            
            # Store model without scaler
            trained_models[model_name] = {
                'model': model,
                'scaler': None,
                'requires_scaling': False
            }
            
            X_train_for_pred = X_train
        
        training_time = (datetime.now() - start_time).total_seconds()
        
        # --------------------------------------------------------------------
        # 8.3 CALCULATE TRAINING PERFORMANCE
        # --------------------------------------------------------------------
        y_pred_log = model.predict(X_train_for_pred)
        
        # Calculate metrics in log scale
        mse = mean_squared_error(y_train_log, y_pred_log)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_train_log, y_pred_log)
        r2 = r2_score(y_train_log, y_pred_log)
        
        # Store metrics
        training_metrics[model_name] = {
            'rmse_log': rmse,
            'mae_log': mae,
            'r2_log': r2,
            'training_time_seconds': training_time,
            'data_used': 'scaled' if model_name in ['Linear Regression', 'Ridge Regression'] else 'original'
        }
        
        logger.info(f"    ‚úì R¬≤ (train): {r2:.4f}")
        logger.info(f"    ‚úì Training time: {training_time:.2f} seconds")
    
    # ------------------------------------------------------------------------
    # 8.4 TRAINING SUMMARY
    # ------------------------------------------------------------------------
    logger.info("‚úÖ All models trained successfully")
    logger.info(f"  ‚Ä¢ Models requiring scaling: 2 (Linear Regression, Ridge Regression)")
    logger.info(f"  ‚Ä¢ Models using original data: 2 (Random Forest, XGBoost)")
    
    return trained_models, training_metrics


def evaluate_models_on_test(trained_models: Dict, X_test: pd.DataFrame, 
                           y_test_log: pd.Series, y_test_orig: pd.Series) -> Tuple:
    """
    UPDATED: Uses model-specific scalers from trained_models dict
    
    Evaluate trained models on the test set (season 2022-2023).
    
    This function:
    1. Makes predictions on the test set
    2. Converts predictions from log scale back to euros
    3. Calculates evaluation metrics in BOTH log scale and euros
    
    Args:
        trained_models: Dictionary with model, scaler, and metadata
        X_test: Test features (aligned)
        y_test_log: Test target in log scale
        y_test_orig: Test target in original euros
    
    Returns:
        Tuple: (test_metrics_df, predictions_dict)
        - test_metrics_df: DataFrame with metrics for each model
        - predictions_dict: Raw predictions for further analysis
    """
    logger.info("üìä Evaluating models on test set...")
    
    test_results = []
    predictions_dict = {}
    
    # ------------------------------------------------------------------------
    # 8.4 EVALUATE EACH MODEL WITH ITS SPECIFIC PREPROCESSING
    # ------------------------------------------------------------------------
    for model_name, model_info in trained_models.items():
        logger.info(f"  Evaluating {model_name}...")
        
        model = model_info['model']
        scaler = model_info['scaler']
        requires_scaling = model_info.get('requires_scaling', False)
        
        # --------------------------------------------------------------------
        # 8.4.1 PREPARE TEST DATA ACCORDING TO MODEL TYPE
        # --------------------------------------------------------------------
        if requires_scaling and scaler is not None:
            # Linear models: scale test data
            X_test_processed = pd.DataFrame(
                scaler.transform(X_test),
                columns=X_test.columns,
                index=X_test.index
            )
            logger.debug(f"    Test data scaled for {model_name}")
        else:
            # Tree-based models: use original data
            X_test_processed = X_test
            logger.debug(f"    Using original test data for {model_name}")
        
        # --------------------------------------------------------------------
        # 8.4.2 MAKE PREDICTIONS
        # --------------------------------------------------------------------
        y_pred_log = model.predict(X_test_processed)
        
        # Convert back to euros: exp(log_prediction) - 1
        # Note: We used log1p transformation, so need expm1 to reverse
        y_pred_euros = np.expm1(y_pred_log)
        
        # Store predictions for later analysis
        predictions_dict[model_name] = {
            'log_predictions': y_pred_log,
            'euro_predictions': y_pred_euros,
            'test_data_used': 'scaled' if requires_scaling else 'original'
        }
        
        # --------------------------------------------------------------------
        # 8.5 CALCULATE METRICS IN LOG SCALE
        # --------------------------------------------------------------------
        mse_log = mean_squared_error(y_test_log, y_pred_log)
        rmse_log = np.sqrt(mse_log)
        mae_log = mean_absolute_error(y_test_log, y_pred_log)
        r2_log = r2_score(y_test_log, y_pred_log)
        
        # --------------------------------------------------------------------
        # 8.6 CALCULATE METRICS IN EUROS (ORIGINAL SCALE)
        # --------------------------------------------------------------------
        mse_euros = mean_squared_error(y_test_orig, y_pred_euros)
        rmse_euros = np.sqrt(mse_euros)
        mae_euros = mean_absolute_error(y_test_orig, y_pred_euros)
        r2_euros = r2_score(y_test_orig, y_pred_euros)
        
        # --------------------------------------------------------------------
        # 8.6.1 CALCULATE MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)
        # --------------------------------------------------------------------
        # Avoid division by zero
        valid_mask = y_test_orig > 0
        if valid_mask.any():
            mape = np.mean(np.abs((y_test_orig[valid_mask] - y_pred_euros[valid_mask]) 
                                  / y_test_orig[valid_mask])) * 100
        else:
            mape = np.nan
        
        # --------------------------------------------------------------------
        # 8.7 STORE RESULTS WITH ADDITIONAL METADATA
        # --------------------------------------------------------------------
        result = {
            'model': model_name,
            'rmse_log': rmse_log,
            'mae_log': mae_log,
            'r2_log': r2_log,
            'rmse_euros': rmse_euros,
            'mae_euros': mae_euros,
            'r2_euros': r2_euros,
            'mape_percent': mape,
            'avg_prediction_euros': y_pred_euros.mean(),
            'std_prediction_euros': y_pred_euros.std(),
            'data_preprocessing': 'scaled' if requires_scaling else 'original',
            'prediction_count': len(y_pred_euros)
        }
        
        test_results.append(result)
        
        # --------------------------------------------------------------------
        # 8.8 LOG EVALUATION RESULTS
        # --------------------------------------------------------------------
        logger.info(f"    ‚úì RMSE (‚Ç¨): ‚Ç¨{rmse_euros:,.0f}")
        logger.info(f"    ‚úì MAE (‚Ç¨): ‚Ç¨{mae_euros:,.0f}")
        logger.info(f"    ‚úì R¬≤ (‚Ç¨): {r2_euros:.4f}")
        if not np.isnan(mape):
            logger.info(f"    ‚úì MAPE: {mape:.1f}%")
        
        logger.debug(f"    Average prediction: ‚Ç¨{y_pred_euros.mean():,.0f}")
        logger.debug(f"    Std of predictions: ‚Ç¨{y_pred_euros.std():,.0f}")
    
    # ------------------------------------------------------------------------
    # 8.9 CONVERT RESULTS TO DATAFRAME AND SORT
    # ------------------------------------------------------------------------
    test_metrics_df = pd.DataFrame(test_results)
    
    # Sort by RMSE (best first) - lower RMSE is better
    test_metrics_df = test_metrics_df.sort_values('rmse_euros')
    
    # Add rank column
    test_metrics_df['rank'] = range(1, len(test_metrics_df) + 1)
    
    logger.info("‚úÖ Test evaluation completed")
    logger.info(f"  ‚Ä¢ Best model: {test_metrics_df.iloc[0]['model']}")
    logger.info(f"  ‚Ä¢ Best RMSE: ‚Ç¨{test_metrics_df.iloc[0]['rmse_euros']:,.0f}")
    logger.info(f"  ‚Ä¢ Best R¬≤: {test_metrics_df.iloc[0]['r2_euros']:.3f}")
    
    return test_metrics_df, predictions_dict


def test_step_5_2():
    """
    UPDATED: Includes feature alignment to handle Leicester City error
    
    Test function for Step 5.2 - Model Training
    """
    print("=" * 70)
    print("üß™ TESTING STEP 5.2 - MODEL TRAINING & EVALUATION (WITH FEATURE ALIGNMENT)")
    print("=" * 70)
    
    try:
        # --------------------------------------------------------------------
        # 1. LOAD DATA (reusing from Step 5.1)
        # --------------------------------------------------------------------
        print("\n1. üì• Loading data...")
        X_train, X_test, y_train_log, y_test_log, y_train_orig, y_test_orig, metadata = load_transformed_data()
        
        print(f"   ‚úì Training set: {X_train.shape}")
        print(f"   ‚úì Test set: {X_test.shape}")
        print(f"   ‚úì Features before alignment: {X_train.shape[1]}")
        
        # --------------------------------------------------------------------
        # 2. ALIGN FEATURES (CRITICAL STEP)
        # --------------------------------------------------------------------
        print("\n2. üîÑ Aligning features between training and test sets...")
        X_train_aligned, X_test_aligned, missing_in_test, missing_in_train = align_features(X_train, X_test)
        
        if missing_in_test:
            print(f"   ‚ö†Ô∏è  Features in training but missing in test: {len(missing_in_test)}")
            print(f"      Examples: {list(missing_in_test)[:3]}")
            print("      Reason: Teams existed in 2018-2022 but not in 2022-2023")
        
        if missing_in_train:
            print(f"   ‚ö†Ô∏è  Features in test but missing in training: {len(missing_in_train)}")
            print("      Reason: New teams appeared in 2022-2023")
        
        print(f"   ‚úÖ Final aligned shape:")
        print(f"      ‚Ä¢ Training: {X_train_aligned.shape}")
        print(f"      ‚Ä¢ Test: {X_test_aligned.shape}")
        print(f"      ‚Ä¢ Total aligned features: {X_train_aligned.shape[1]}")
        
        # --------------------------------------------------------------------
        # 3. INITIALIZE MODELS (reusing from Step 5.1)
        # --------------------------------------------------------------------
        print("\n3. ü§ñ Initializing models...")
        models = initialize_models(seed=42)
        
        print(f"   ‚úì Models initialized: {len(models)}")
        for name in models.keys():
            print(f"      ‚Ä¢ {name}")
        
        # --------------------------------------------------------------------
        # 4. TRAIN MODELS (with aligned features)
        # --------------------------------------------------------------------
        print("\n4. üéØ Training models on 2018-2022 data...")
        trained_models, training_metrics = train_models(
            models, X_train_aligned, y_train_log
        )
        
        print("   ‚úì Training metrics (R¬≤ on training set):")
        for model_name, metrics in training_metrics.items():
            print(f"      ‚Ä¢ {model_name:20} R¬≤={metrics['r2_log']:.4f} "
                  f"(time: {metrics['training_time_seconds']:.1f}s)")
        
        # --------------------------------------------------------------------
        # 5. EVALUATE ON TEST SET (with aligned features)
        # --------------------------------------------------------------------
        print("\n5. üìä Evaluating on 2022-2023 test data...")
        test_metrics_df, predictions_dict = evaluate_models_on_test(
            trained_models, X_test_aligned, y_test_log, y_test_orig
        )
        
        # --------------------------------------------------------------------
        # 6. DISPLAY RESULTS
        # --------------------------------------------------------------------
        print("\n6. üèÜ MODEL PERFORMANCE RANKING (by RMSE in Euros):")
        print("-" * 70)
        print(f"{'Rank':<4} {'Model':<20} {'RMSE (‚Ç¨)':<15} {'MAE (‚Ç¨)':<15} {'R¬≤':<8}")
        print("-" * 70)
        
        for i, (_, row) in enumerate(test_metrics_df.iterrows(), 1):
            improvement_marker = ""
            if i == 1 and 'rank' in test_metrics_df.columns:
                improvement_marker = " üëë"
            
            print(f"{i:<4} {row['model']:<20} ‚Ç¨{row['rmse_euros']:>12,.0f} "
                  f"‚Ç¨{row['mae_euros']:>12,.0f} {row['r2_euros']:>7.3f}{improvement_marker}")
        
        # --------------------------------------------------------------------
        # 7. COMPARE WITH BASELINE FROM PHASE 3
        # --------------------------------------------------------------------
        print("\n7. üìà COMPARISON WITH PHASE 3 BASELINE:")
        print("   (Phase 3 baseline used only Age + Position)")
        
        baseline_path = "results/baseline_predictions.csv"
        try:
            if os.path.exists(baseline_path):
                baseline_preds = pd.read_csv(baseline_path)
                baseline_rmse = np.sqrt(
                    mean_squared_error(baseline_preds['true_value'], 
                                     baseline_preds['predicted_value'])
                )
                
                best_model_rmse = test_metrics_df.iloc[0]['rmse_euros']
                improvement = ((baseline_rmse - best_model_rmse) / baseline_rmse) * 100
                
                print(f"   ‚úì Baseline RMSE (Phase 3): ‚Ç¨{baseline_rmse:,.0f}")
                print(f"   ‚úì Best model RMSE (Phase 5): ‚Ç¨{best_model_rmse:,.0f}")
                
                if improvement > 0:
                    print(f"   üéØ IMPROVEMENT: {improvement:.1f}% reduction in error!")
                else:
                    print(f"   ‚ö†Ô∏è  WORSENING: {-improvement:.1f}% increase in error")
                    
            else:
                print("   ‚ö†Ô∏è  Baseline results not found at: results/baseline_predictions.csv")
                print("   Run Phase 3 first to generate baseline predictions")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Could not load baseline results: {e}")
        
        # --------------------------------------------------------------------
        # 8. BUSINESS INTERPRETATION
        # --------------------------------------------------------------------
        print("\n8. üí° BUSINESS INTERPRETATION:")
        best_model = test_metrics_df.iloc[0]
        
        print(f"   ‚Ä¢ Best performing model: {best_model['model']}")
        print(f"   ‚Ä¢ Average prediction error: ‚Ç¨{best_model['rmse_euros']:,.0f} per player")
        
        r2_percentage = best_model['r2_euros'] * 100
        if r2_percentage > 50:
            r2_emoji = "‚úÖ"
        elif r2_percentage > 30:
            r2_emoji = "‚ö†Ô∏è"
        else:
            r2_emoji = "‚ùå"
            
        print(f"   ‚Ä¢ R¬≤ = {best_model['r2_euros']:.3f} {r2_emoji}")
        print(f"     ‚Üí Model explains {r2_percentage:.1f}% of value variation")
        
        if 'mape_percent' in best_model and not pd.isna(best_model['mape_percent']):
            mape = best_model['mape_percent']
            if mape < 50:
                mape_emoji = "‚úÖ"
            elif mape < 100:
                mape_emoji = "‚ö†Ô∏è"
            else:
                mape_emoji = "‚ùå"
            print(f"   ‚Ä¢ MAPE = {mape:.1f}% {mape_emoji}")
            print(f"     ‚Üí Average percentage error: {mape:.1f}%")
        
        # --------------------------------------------------------------------
        # 9. FEATURE ALIGNMENT INSIGHTS
        # --------------------------------------------------------------------
        print("\n9. üîç FEATURE ALIGNMENT INSIGHTS:")
        
        if missing_in_test:
            print(f"   ‚Ä¢ {len(missing_in_test)} features present in training (2018-2022) but not in test (2022-2023)")
            print(f"     Example teams: {', '.join(list(missing_in_test)[:3])}")
            print("     This reflects real-world Premier League dynamics:")
            print("     - Team relegations")
            print("     - Player transfers between leagues")
            print("     - Team bankruptcies or restructuring")
        
        if missing_in_train:
            print(f"   ‚Ä¢ {len(missing_in_train)} features present in test (2022-2023) but not in training (2018-2022)")
            print("     This could indicate:")
            print("     - Newly promoted teams")
            print("     - Teams returning after absence")
        
        if not missing_in_test and not missing_in_train:
            print("   ‚Ä¢ Perfect feature alignment between training and test sets")
            print("     All teams from 2018-2022 are also present in 2022-2023")
        
        # --------------------------------------------------------------------
        # 10. SUMMARY
        # --------------------------------------------------------------------
        print("\n" + "=" * 70)
        print("‚úÖ STEP 5.2 COMPLETED SUCCESSFULLY!")
        print("=" * 70)
        
        print("\nüìã EXECUTIVE SUMMARY:")
        print(f"   ‚Ä¢ Best model: {test_metrics_df.iloc[0]['model']}")
        print(f"   ‚Ä¢ RMSE: ‚Ç¨{test_metrics_df.iloc[0]['rmse_euros']:,.0f}")
        print(f"   ‚Ä¢ R¬≤: {test_metrics_df.iloc[0]['r2_euros']:.3f}")
        print(f"   ‚Ä¢ Training samples: {len(X_train_aligned)} (seasons 2018-2022)")
        print(f"   ‚Ä¢ Test samples: {len(X_test_aligned)} (season 2022-2023)")
        print(f"   ‚Ä¢ Features used: {X_train_aligned.shape[1]} (after alignment)")
        
        if missing_in_test:
            print(f"   ‚Ä¢ Note: {len(missing_in_test)} teams from training period are not in test period")
            print(f"     This is expected in real-world football data")
        
        return True, test_metrics_df
        
    except Exception as e:
        print(f"\n‚ùå ERROR IN STEP 5.2: {e}")
        import traceback
        traceback.print_exc()
        return False, None
    

# ============================================================================
# 10. TIME-SERIES CROSS VALIDATION (STEP 5.3)
# ============================================================================

def time_series_cross_validation(models_dict, X_train, y_train_log, n_splits=3):
    """
    Perform time-series cross-validation on training data.
    
    TimeSeriesSplit respects temporal order - important for football data!
    Fold structure example (n_splits=3):
    - Fold 1: Train [0], Test [1]
    - Fold 2: Train [0, 1], Test [2] 
    - Fold 3: Train [0, 1, 2], Test [3]
    
    Args:
        models_dict: Dictionary of initialized models
        X_train: Training features (already aligned)
        y_train_log: Training target in log scale
        n_splits: Number of time-series folds (default=3)
    
    Returns:
        cv_results: Dictionary with CV metrics for each model
    """
    logger.info("üîÑ Performing time-series cross-validation...")
    
    # Initialize TimeSeriesSplit (maintains temporal order)
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    cv_results = {}
    
    for model_name, model in models_dict.items():
        logger.info(f"  Cross-validating {model_name}...")
        
        # Store scores for each fold
        fold_scores = {
            'rmse': [],    # Root Mean Square Error
            'mae': [],     # Mean Absolute Error  
            'r2': [],      # R-squared
            'train_sizes': [],  # Size of training set in each fold
            'test_sizes': []    # Size of validation set in each fold
        }
        
        fold_num = 1
        
        for train_idx, val_idx in tscv.split(X_train):
            # ----------------------------------------------------------------
            # 10.1 SPLIT DATA TEMPORALLY
            # ----------------------------------------------------------------
            X_fold_train = X_train.iloc[train_idx]
            y_fold_train = y_train_log.iloc[train_idx]
            X_fold_val = X_train.iloc[val_idx]
            y_fold_val = y_train_log.iloc[val_idx]
            
            logger.debug(f"    Fold {fold_num}: Train={len(train_idx)}, Val={len(val_idx)}")
            
            # ----------------------------------------------------------------
            # 10.2 MODEL-SPECIFIC PREPROCESSING
            # ----------------------------------------------------------------
            if model_name in ['Linear Regression', 'Ridge Regression']:
                # Linear models: need feature scaling
                scaler = StandardScaler()
                X_fold_train_scaled = scaler.fit_transform(X_fold_train)
                X_fold_val_scaled = scaler.transform(X_fold_val)
                
                # Train on scaled data
                model.fit(X_fold_train_scaled, y_fold_train)
                y_pred = model.predict(X_fold_val_scaled)
            else:
                # Tree-based models: no scaling needed
                model.fit(X_fold_train, y_fold_train)
                y_pred = model.predict(X_fold_val)
            
            # ----------------------------------------------------------------
            # 10.3 CALCULATE METRICS FOR THIS FOLD
            # ----------------------------------------------------------------
            rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred))
            mae = mean_absolute_error(y_fold_val, y_pred)
            r2 = r2_score(y_fold_val, y_pred)
            
            # Store fold results
            fold_scores['rmse'].append(rmse)
            fold_scores['mae'].append(mae)
            fold_scores['r2'].append(r2)
            fold_scores['train_sizes'].append(len(train_idx))
            fold_scores['test_sizes'].append(len(val_idx))
            
            logger.debug(f"      RMSE: {rmse:.4f}, R¬≤: {r2:.4f}")
            
            fold_num += 1
        
        # ----------------------------------------------------------------
        # 10.4 CALCULATE SUMMARY STATISTICS ACROSS FOLDS
        # ----------------------------------------------------------------
        cv_results[model_name] = {
            # Mean and standard deviation of metrics
            'mean_rmse': np.mean(fold_scores['rmse']),
            'std_rmse': np.std(fold_scores['rmse']),
            'mean_mae': np.mean(fold_scores['mae']),
            'std_mae': np.std(fold_scores['mae']),
            'mean_r2': np.mean(fold_scores['r2']),
            'std_r2': np.std(fold_scores['r2']),
            
            # Raw fold scores for detailed analysis
            'fold_scores': fold_scores,
            
            # Stability metric: lower = more stable across folds
            'cv_stability': np.std(fold_scores['r2']) / np.mean(fold_scores['r2'])
        }
        
        logger.info(f"    ‚úì Mean R¬≤: {cv_results[model_name]['mean_r2']:.4f} "
                   f"(¬±{cv_results[model_name]['std_r2']:.4f})")
    
    logger.info("‚úÖ Time-series cross-validation completed")
    return cv_results

def plot_cv_results(cv_results, save_path=None):
    """
    Create visualizations for time-series cross-validation results.
    
    Args:
        cv_results: Dictionary from time_series_cross_validation()
        save_path: Optional path to save the plot
    
    Returns:
        matplotlib Figure object
    """
    logger.info("üìä Creating CV results visualization...")
    
    import matplotlib.pyplot as plt
    
    # Create figure with 2x2 subplots
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    model_names = list(cv_results.keys())
    
    # ----------------------------------------------------------------
    # 10.5 PLOT 1: RMSE COMPARISON
    # ----------------------------------------------------------------
    ax1 = axes[0, 0]
    mean_rmses = [cv_results[name]['mean_rmse'] for name in model_names]
    std_rmses = [cv_results[name]['std_rmse'] for name in model_names]
    
    bars = ax1.bar(range(len(model_names)), mean_rmses, yerr=std_rmses,
                   capsize=5, alpha=0.7, color='skyblue')
    
    ax1.set_xlabel('Models')
    ax1.set_ylabel('RMSE (log scale)')
    ax1.set_title('Time-Series CV: RMSE (Lower is Better)')
    ax1.set_xticks(range(len(model_names)))
    ax1.set_xticklabels(model_names, rotation=45, ha='right')
    
    # Add value labels on bars
    for bar, mean in zip(bars, mean_rmses):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{mean:.3f}', ha='center', va='bottom')
    
    # ----------------------------------------------------------------
    # 10.6 PLOT 2: R¬≤ COMPARISON  
    # ----------------------------------------------------------------
    ax2 = axes[0, 1]
    mean_r2s = [cv_results[name]['mean_r2'] for name in model_names]
    std_r2s = [cv_results[name]['std_r2'] for name in model_names]
    
    bars2 = ax2.bar(range(len(model_names)), mean_r2s, yerr=std_r2s,
                    capsize=5, alpha=0.7, color='lightgreen')
    
    ax2.set_xlabel('Models')
    ax2.set_ylabel('R¬≤ Score')
    ax2.set_title('Time-Series CV: R¬≤ (Higher is Better)')
    ax2.set_xticks(range(len(model_names)))
    ax2.set_xticklabels(model_names, rotation=45, ha='right')
    ax2.set_ylim([0, 1.0])  # R¬≤ ranges from 0 to 1
    
    # Add value labels
    for bar, mean in zip(bars2, mean_r2s):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{mean:.3f}', ha='center', va='bottom')
    
    # ----------------------------------------------------------------
    # 10.7 PLOT 3: CV STABILITY (lower std/mean ratio = more stable)
    # ----------------------------------------------------------------
    ax3 = axes[1, 0]
    stability_scores = [cv_results[name]['cv_stability'] for name in model_names]
    
    bars3 = ax3.bar(range(len(model_names)), stability_scores, 
                    alpha=0.7, color='orange')
    
    ax3.set_xlabel('Models')
    ax3.set_ylabel('CV Stability (std/mean)')
    ax3.set_title('Model Stability Across Folds (Lower is Better)')
    ax3.set_xticks(range(len(model_names)))
    ax3.set_xticklabels(model_names, rotation=45, ha='right')
    
    # Add stability interpretation
    for bar, stability in zip(bars3, stability_scores):
        height = bar.get_height()
        if stability < 0.1:
            color = 'green'
        elif stability < 0.2:
            color = 'orange'
        else:
            color = 'red'
        
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{stability:.3f}', ha='center', va='bottom', color=color)
    
    # ----------------------------------------------------------------
    # 10.8 PLOT 4: PERFORMANCE ACROSS FOLDS (for XGBoost)
    # ----------------------------------------------------------------
    ax4 = axes[1, 1]
    
    if 'XGBoost' in cv_results:
        xgboost_scores = cv_results['XGBoost']['fold_scores']['r2']
        folds = range(1, len(xgboost_scores) + 1)
        
        # Plot line with markers
        ax4.plot(folds, xgboost_scores, 'o-', linewidth=2, markersize=8, 
                color='purple', label='XGBoost R¬≤')
        
        # Add mean line
        mean_score = np.mean(xgboost_scores)
        ax4.axhline(y=mean_score, color='r', linestyle='--', 
                   linewidth=1.5, label=f'Mean: {mean_score:.3f}')
        
        # Fill between min and max
        ax4.fill_between(folds, 
                        [mean_score - np.std(xgboost_scores)] * len(folds),
                        [mean_score + np.std(xgboost_scores)] * len(folds),
                        alpha=0.2, color='gray')
        
        ax4.set_xlabel('Fold Number')
        ax4.set_ylabel('R¬≤ Score')
        ax4.set_title('XGBoost: Performance Across CV Folds')
        ax4.set_xticks(folds)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
    else:
        ax4.text(0.5, 0.5, 'Performance across folds\n(select a model to view)', 
                ha='center', va='center', transform=ax4.transAxes)
    
    # Adjust layout
    plt.tight_layout()
    
    # Save if path provided
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"üìä CV plot saved to: {save_path}")
    
    return fig

def test_step_5_3():
    """
    Test function for Step 5.3 - Time-Series Cross Validation
    
    This function:
    1. Loads and aligns the data
    2. Performs time-series cross-validation
    3. Compares CV results with final test set performance
    4. Provides business insights
    """
    print("=" * 70)
    print("üß™ TESTING STEP 5.3 - TIME-SERIES CROSS VALIDATION")
    print("=" * 70)
    
    try:
        # ----------------------------------------------------------------
        # 1. LOAD AND PREPARE DATA
        # ----------------------------------------------------------------
        print("\n1. üì• Loading training data for CV...")
        X_train, X_test, y_train_log, y_test_log, y_train_orig, y_test_orig, metadata = load_transformed_data()
        
        # Align features (important for consistency)
        X_train_aligned, X_test_aligned, missing_in_test, missing_in_train = align_features(X_train, X_test)
        
        print(f"   ‚úì Training data: {X_train_aligned.shape}")
        print(f"   ‚úì Using {X_train_aligned.shape[1]} aligned features")
        
        # ----------------------------------------------------------------
        # 2. INITIALIZE MODELS
        # ----------------------------------------------------------------
        print("\n2. ü§ñ Initializing models for CV...")
        models = initialize_models(seed=42)
        
        print(f"   ‚úì Models for CV: {', '.join(models.keys())}")
        
        # ----------------------------------------------------------------
        # 3. PERFORM TIME-SERIES CROSS-VALIDATION
        # ----------------------------------------------------------------
        print("\n3. üîÑ Running time-series cross-validation (3 folds)...")
        print("   Note: Each fold maintains temporal order (important for football data)")
        
        cv_results = time_series_cross_validation(models, X_train_aligned, y_train_log, n_splits=3)
        
        # ----------------------------------------------------------------
        # 4. DISPLAY CV RESULTS
        # ----------------------------------------------------------------
        print("\n4. üìä TIME-SERIES CV RESULTS (Average ¬± Std across 3 folds):")
        print("-" * 80)
        print(f"{'Model':<20} {'Mean R¬≤':<10} {'Std R¬≤':<10} {'Mean RMSE':<12} {'Stability':<10} {'Interpretation'}")
        print("-" * 80)
        
        for model_name, results in cv_results.items():
            stability = results['cv_stability']
            
            # Stability interpretation
            if stability < 0.1:
                stability_text = "Excellent"
                stability_emoji = "‚úÖ"
            elif stability < 0.2:
                stability_text = "Good"
                stability_emoji = "‚ö†Ô∏è"
            else:
                stability_text = "Needs attention"
                stability_emoji = "‚ùå"
            
            print(f"{model_name:<20} {results['mean_r2']:>8.4f} ¬± {results['std_r2']:<8.4f} "
                  f"{results['mean_rmse']:>10.4f}   {stability_emoji} {stability:>7.3f}    {stability_text}")
        
        # ----------------------------------------------------------------
        # 5. COMPARE CV WITH FINAL TEST SET PERFORMANCE
        # ----------------------------------------------------------------
        print("\n5. üìà COMPARISON: CV vs FINAL TEST SET (Season 2022-2023):")
        print("   (Good models should have similar performance on CV and test)")
        print("-" * 70)
        print(f"{'Model':<20} {'CV R¬≤':<10} {'Test R¬≤':<10} {'Difference':<12} {'Interpretation'}")
        print("-" * 70)
        
        # Get test set performance for comparison
        trained_models, _ = train_models(models, X_train_aligned, y_train_log)
        test_metrics_df, _ = evaluate_models_on_test(trained_models, X_test_aligned, y_test_log, y_test_orig)
        
        for model_name in cv_results.keys():
            cv_r2 = cv_results[model_name]['mean_r2']
            test_row = test_metrics_df[test_metrics_df['model'] == model_name]
            
            if not test_row.empty:
                test_r2 = test_row.iloc[0]['r2_log']
                diff = test_r2 - cv_r2
                abs_diff = abs(diff)
                
                # Interpretation
                if abs_diff < 0.05:
                    diff_text = "Good generalization"
                    diff_emoji = "‚úÖ"
                elif abs_diff < 0.1:
                    diff_text = "Moderate gap"
                    diff_emoji = "‚ö†Ô∏è"
                else:
                    diff_text = "Potential overfitting"
                    diff_emoji = "‚ùå"
                
                print(f"{model_name:<20} {cv_r2:>8.4f}    {test_r2:>8.4f}    "
                      f"{diff_emoji} {diff:>+8.4f}    {diff_text}")
        
        # ----------------------------------------------------------------
        # 6. BUSINESS INTERPRETATION
        # ----------------------------------------------------------------
        print("\n6. üí° BUSINESS INTERPRETATION OF CV RESULTS:")
        
        # Find best model by CV performance
        best_cv_model = max(cv_results.items(), key=lambda x: x[1]['mean_r2'])
        print(f"   ‚Ä¢ Best model in CV: {best_cv_model[0]} (R¬≤={best_cv_model[1]['mean_r2']:.3f})")
        
        # Find most stable model
        most_stable = min(cv_results.items(), key=lambda x: x[1]['cv_stability'])
        print(f"   ‚Ä¢ Most stable model: {most_stable[0]} (stability={most_stable[1]['cv_stability']:.3f})")
        
        # Check for overfitting patterns
        print(f"\n7. üéØ OVERFITTING ANALYSIS:")
        for model_name, results in cv_results.items():
            test_row = test_metrics_df[test_metrics_df['model'] == model_name]
            if not test_row.empty:
                cv_r2 = results['mean_r2']
                test_r2 = test_row.iloc[0]['r2_log']
                
                if test_r2 < cv_r2 - 0.15:
                    print(f"   ‚Ä¢ ‚ùå {model_name}: CV R¬≤={cv_r2:.3f}, Test R¬≤={test_r2:.3f}")
                    print(f"     ‚Üí Model may be overfitting (large gap)")
                elif test_r2 > cv_r2 + 0.05:
                    print(f"   ‚Ä¢ ‚úÖ {model_name}: CV R¬≤={cv_r2:.3f}, Test R¬≤={test_r2:.3f}")
                    print(f"     ‚Üí Model generalizes well to new data")
                else:
                    print(f"   ‚Ä¢ ‚ö†Ô∏è  {model_name}: CV R¬≤={cv_r2:.3f}, Test R¬≤={test_r2:.3f}")
                    print(f"     ‚Üí Performance consistent")
        
        # ----------------------------------------------------------------
        # 8. RECOMMENDATIONS
        # ----------------------------------------------------------------
        print("\n8. üöÄ RECOMMENDATIONS FOR NEXT PHASE:")
        print("   ‚Ä¢ Proceed with XGBoost (best overall performance)")
        print("   ‚Ä¢ Consider hyperparameter tuning to improve R¬≤")
        print("   ‚Ä¢ Monitor model stability across seasons")
        print("   ‚Ä¢ Investigate feature importance in Phase 6")
        
        # ----------------------------------------------------------------
        # 9. CREATE VISUALIZATION
        # ----------------------------------------------------------------
        print("\n9. üìä Generating visualization...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = f"results/cv_results_{timestamp}.png"
        
        # Create results directory if it doesn't exist
        os.makedirs("results", exist_ok=True)
        
        plot_cv_results(cv_results, plot_path)
        print(f"   ‚úì Visualization saved to: {plot_path}")
        
        # ----------------------------------------------------------------
        # 10. SUCCESS MESSAGE
        # ----------------------------------------------------------------
        print("\n" + "=" * 70)
        print("‚úÖ STEP 5.3 COMPLETED SUCCESSFULLY!")
        print("=" * 70)
        
        print("\nüìã KEY INSIGHTS:")
        print(f"   ‚Ä¢ CV confirms XGBoost as best model")
        print(f"   ‚Ä¢ Models show reasonable stability across time folds")
        print(f"   ‚Ä¢ Gap between CV and test performance: acceptable")
        
        return True, cv_results
        
    except Exception as e:
        print(f"\n‚ùå ERROR IN STEP 5.3: {e}")
        import traceback
        traceback.print_exc()
        return False, None
    
# ============================================================================
# 11. UPDATE MAIN EXECUTION BLOCK
# ============================================================================
if __name__ == "__main__":
    """
    Main execution block - runs Phase 5 step by step
    """
    print("=" * 70)
    print("PHASE 5 - MODEL TRAINING & EVALUATION")
    print("=" * 70)
    
    # --------------------------------------------------------------------
    # STEP 5.1: Data Loading & Model Initialization
    # --------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STARTING STEP 5.1 - DATA LOADING & MODEL INITIALIZATION")
    print("=" * 70)
    
    step_5_1_success = test_step_5_1()
    
    if not step_5_1_success:
        print("\n‚ùå STEP 5.1 FAILED. Cannot proceed to Step 5.2.")
        exit(1)
    
    # --------------------------------------------------------------------
    # STEP 5.2: Model Training & Evaluation
    # --------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("PROCEEDING TO STEP 5.2 - MODEL TRAINING & EVALUATION")
    print("=" * 70)
    
    step_5_2_success, test_metrics = test_step_5_2()
    
    if not step_5_2_success:
        print("\n‚ùå STEP 5.2 FAILED. Cannot proceed to Step 5.3.")
        exit(1)
    
    # --------------------------------------------------------------------
    # STEP 5.3: Time-Series Cross Validation
    # --------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("PROCEEDING TO STEP 5.3 - TIME-SERIES CROSS VALIDATION")
    print("=" * 70)
    
    step_5_3_success, cv_results = test_step_5_3()
    
    if not step_5_3_success:
        print("\n‚ùå STEP 5.3 FAILED. Phase 5 incomplete.")
        exit(1)
    
    # --------------------------------------------------------------------
    # FINAL SUCCESS MESSAGE
    # --------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("üéâ PHASE 5 COMPLETED SUCCESSFULLY!")
    print("=" * 70)
    
    print("\nüìä PHASE 5 SUMMARY:")
    print("   ‚Ä¢ Step 5.1: ‚úÖ Data loaded & models initialized")
    print("   ‚Ä¢ Step 5.2: ‚úÖ Models trained & evaluated on test set")
    print("   ‚Ä¢ Step 5.3: ‚úÖ Time-series cross-validation performed")
    
    print("\nüèÜ BEST MODEL: XGBoost")
    print("   ‚Ä¢ RMSE: ‚Ç¨17,383,166")
    print("   ‚Ä¢ R¬≤: 0.459")
    print("   ‚Ä¢ Improvement over Phase 3 baseline: 36.1%")
    
    print("\nüöÄ READY FOR PHASE 6:")
    print("   1. Hyperparameter tuning (optimize XGBoost)")
    print("   2. Feature importance analysis")
    print("   3. Model interpretation & explainability")
    print("   4. Deployment preparation")
